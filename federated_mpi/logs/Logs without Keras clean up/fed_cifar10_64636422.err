2025-07-24 08:42:55.146743: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:42:55.146705: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:42:55.146603: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:42:55.148096: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:42:55.147146: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:42:55.146833: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:42:55.146398: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:42:55.146870: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:42:55.146688: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:42:55.146520: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:42:55.146455: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:42:55.146654: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:42:55.146610: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:42:55.146790: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:42:55.148480: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:42:55.167091: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:42:55.832054: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:43:04.641698: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:43:04.873179: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:43:04.963269: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:43:05.014996: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:43:05.171808: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:43:06.162198: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:43:10.815969: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:43:11.323673: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:43:11.323766: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:43:11.323827: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:43:11.323976: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:43:11.323828: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:43:11.323916: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:43:11.324111: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:43:11.324245: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:43:11.324812: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:43:11.324725: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:43:11.324813: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:43:11.324858: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:43:11.324860: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:43:11.324862: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:43:11.324859: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:43:11.324859: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:43:11.324884: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:43:11.325081: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:43:11.324964: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:43:11.326441: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:43:11.328620: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:43:11.333344: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:43:11.347946: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:43:12.420311: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:43:12.468514: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:43:12.679701: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:43:12.680380: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:43:12.680594: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:43:12.680663: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:43:12.680847: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:43:12.680941: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:43:12.681213: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:43:12.681336: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:43:12.681685: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:43:12.682352: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:43:12.682411: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:43:12.682413: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:43:12.682605: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:43:12.682570: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:43:12.682804: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:43:12.683176: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:43:12.683502: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:43:12.695339: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:43:12.695555: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:43:12.705270: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:43:12.805224: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:43:12.961378: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:43:13.051654: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:43:13.059628: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:43:13.163646: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:43:13.592345: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:43:13.592581: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:43:13.592599: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:43:13.592686: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:43:13.592789: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:43:13.592807: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:43:13.592835: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:43:13.592824: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:43:13.592812: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:43:13.592928: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:43:13.593035: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:43:13.593040: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:43:13.593167: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:43:13.593223: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:43:13.593292: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:43:13.593385: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:43:13.593356: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:43:13.593572: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:43:13.593515: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:43:13.593693: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:43:13.593569: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:43:13.593409: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:43:13.594666: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:43:13.838043: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:43:14.158440: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:43:14.502823: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:43:14.624998: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:43:14.718980: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:43:14.759851: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:43:14.771379: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:43:14.975737: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:43:15.016689: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:43:15.095468: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:43:15.168700: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:43:15.274767: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:43:15.288810: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:43:15.360973: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:43:15.422719: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:43:15.513843: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:43:15.592598: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:43:15.706409: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:43:15.966983: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:43:17.110176: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:43:17.110243: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:43:17.110179: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:43:17.110237: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:43:17.110370: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:43:17.110645: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:43:17.110693: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:43:17.111167: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:43:17.111362: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:43:17.115417: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:43:17.119171: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:43:17.119728: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:43:17.165052: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:43:17.269365: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:43:17.328652: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:43:17.340454: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:43:17.393214: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:43:17.417586: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:43:17.552386: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:43:17.587396: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:43:17.654228: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:43:17.732180: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:43:17.734741: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:43:17.737331: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:43:17.975912: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:43:17.981570: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:43:18.022261: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:43:18.103225: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:43:18.595299: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:43:19.165511: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:43:19.329719: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:43:20.710493: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:43:21.670445: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:43:22.342905: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:43:22.366197: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:43:22.449441: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:43:22.933003: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:43:25.864359: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:43:26.694743: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:44:19.970596: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:44:20.048719: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:44:20.063859: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:44:20.090944: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:44:20.100752: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:44:20.115288: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:44:20.205693: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:44:20.213185: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:44:20.219497: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:44:20.240100: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:44:20.248800: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:44:20.250442: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:44:20.313663: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:44:20.332772: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:44:20.345707: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:44:20.394092: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:44:20.636756: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:44:20.846597: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:44:20.893818: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:44:20.970500: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:44:21.252751: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:44:21.554673: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:44:21.616398: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:44:21.626428: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:44:21.895596: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:44:21.927584: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:44:22.042137: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:44:22.252971: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:44:22.632524: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:44:27.040604: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:44:40.249517: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
Traceback (most recent call last):
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 15, in <module>
Traceback (most recent call last):
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 15, in <module>
Traceback (most recent call last):
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 15, in <module>
Traceback (most recent call last):
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 15, in <module>
Traceback (most recent call last):
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 15, in <module>
Traceback (most recent call last):
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 15, in <module>
Traceback (most recent call last):
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 15, in <module>
Traceback (most recent call last):
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 15, in <module>
Traceback (most recent call last):
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 15, in <module>
Traceback (most recent call last):
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 15, in <module>
    main()
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 12, in main
Traceback (most recent call last):
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 15, in <module>
Traceback (most recent call last):
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 15, in <module>
Traceback (most recent call last):
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 15, in <module>
Traceback (most recent call last):
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 15, in <module>
    main()
Traceback (most recent call last):
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 15, in <module>
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 12, in main
Traceback (most recent call last):
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 15, in <module>
Traceback (most recent call last):
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 15, in <module>
    main()
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 12, in main
    run_client(comm, rank)
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_client.py", line 69, in run_client
Traceback (most recent call last):
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 15, in <module>
    run_client(comm, rank)
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_client.py", line 69, in run_client
    main()
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 12, in main
    run_client(comm, rank)
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_client.py", line 69, in run_client
    main()
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 12, in main
    main()
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 9, in main
    run_client(comm, rank)
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_client.py", line 69, in run_client
    main()
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 12, in main
    run_client(comm, rank)
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_client.py", line 69, in run_client
    main()
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 12, in main
    main()
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 12, in main
    run_client(comm, rank)
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_client.py", line 69, in run_client
    global_weights = comm.bcast(None, root=0)
                     ^^^^^^^^^^^^^^^^^^^^^^^^
  File "src/mpi4py/MPI.src/Comm.pyx", line 2113, in mpi4py.MPI.Comm.bcast
    run_client(comm, rank)
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_client.py", line 69, in run_client
    run_client(comm, rank)
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_client.py", line 69, in run_client
    main()
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 12, in main
    main()
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 12, in main
    global_weights = comm.bcast(None, root=0)
                     ^^^^^^^^^^^^^^^^^^^^^^^^
  File "src/mpi4py/MPI.src/Comm.pyx", line 2113, in mpi4py.MPI.Comm.bcast
    run_client(comm, rank)
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_client.py", line 69, in run_client
    main()
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 12, in main
    run_server(comm)
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_server.py", line 46, in run_server
    main()
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 12, in main
    global_weights = comm.bcast(None, root=0)
                     ^^^^^^^^^^^^^^^^^^^^^^^^
  File "src/mpi4py/MPI.src/Comm.pyx", line 2113, in mpi4py.MPI.Comm.bcast
    run_client(comm, rank)
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_client.py", line 69, in run_client
    main()
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 12, in main
    run_client(comm, rank)
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_client.py", line 69, in run_client
    main()
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 12, in main
    run_client(comm, rank)
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_client.py", line 69, in run_client
    main()
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 12, in main
    run_client(comm, rank)
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_client.py", line 69, in run_client
    run_client(comm, rank)
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_client.py", line 69, in run_client
    main()
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 12, in main
    run_client(comm, rank)
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_client.py", line 69, in run_client
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 776, in mpi4py.MPI.PyMPI_bcast
    global_weights = comm.bcast(None, root=0)
                     ^^^^^^^^^^^^^^^^^^^^^^^^
  File "src/mpi4py/MPI.src/Comm.pyx", line 2113, in mpi4py.MPI.Comm.bcast
    global_weights = comm.bcast(None, root=0)
    global_weights = comm.bcast(None, root=0)
                     ^^^^^^^^^^^^^^^^^^^^^^^^
  File "src/mpi4py/MPI.src/Comm.pyx", line 2113, in mpi4py.MPI.Comm.bcast
                     ^^^^^^^^^^^^^^^^^^^^^^^^
  File "src/mpi4py/MPI.src/Comm.pyx", line 2113, in mpi4py.MPI.Comm.bcast
    global_weights = comm.bcast(None, root=0)
                     ^^^^^^^^^^^^^^^^^^^^^^^^
  File "src/mpi4py/MPI.src/Comm.pyx", line 2113, in mpi4py.MPI.Comm.bcast
    run_client(comm, rank)
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_client.py", line 69, in run_client
    comm.bcast(global_weights, root=0)
  File "src/mpi4py/MPI.src/Comm.pyx", line 2113, in mpi4py.MPI.Comm.bcast
    global_weights = comm.bcast(None, root=0)
                     ^^^^^^^^^^^^^^^^^^^^^^^^
  File "src/mpi4py/MPI.src/Comm.pyx", line 2113, in mpi4py.MPI.Comm.bcast
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 206, in mpi4py.MPI.pickle_load
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 776, in mpi4py.MPI.PyMPI_bcast
    global_weights = comm.bcast(None, root=0)
                     ^^^^^^^^^^^^^^^^^^^^^^^^
  File "src/mpi4py/MPI.src/Comm.pyx", line 2113, in mpi4py.MPI.Comm.bcast
    main()
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 12, in main
    global_weights = comm.bcast(None, root=0)
                     ^^^^^^^^^^^^^^^^^^^^^^^^
  File "src/mpi4py/MPI.src/Comm.pyx", line 2113, in mpi4py.MPI.Comm.bcast
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 195, in mpi4py.MPI.cloads
    run_client(comm, rank)
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_client.py", line 69, in run_client
    global_weights = comm.bcast(None, root=0)
                     ^^^^^^^^^^^^^^^^^^^^^^^^
  File "src/mpi4py/MPI.src/Comm.pyx", line 2113, in mpi4py.MPI.Comm.bcast
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 776, in mpi4py.MPI.PyMPI_bcast
_pickle.UnpicklingError: invalid load key, '\x00'.
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 776, in mpi4py.MPI.PyMPI_bcast
    global_weights = comm.bcast(None, root=0)
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 776, in mpi4py.MPI.PyMPI_bcast
                     ^^^^^^^^^^^^^^^^^^^^^^^^
  File "src/mpi4py/MPI.src/Comm.pyx", line 2113, in mpi4py.MPI.Comm.bcast
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 206, in mpi4py.MPI.pickle_load
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 206, in mpi4py.MPI.pickle_load
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 776, in mpi4py.MPI.PyMPI_bcast
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 776, in mpi4py.MPI.PyMPI_bcast
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 195, in mpi4py.MPI.cloads
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 776, in mpi4py.MPI.PyMPI_bcast
    global_weights = comm.bcast(None, root=0)
                     ^^^^^^^^^^^^^^^^^^^^^^^^
  File "src/mpi4py/MPI.src/Comm.pyx", line 2113, in mpi4py.MPI.Comm.bcast
    global_weights = comm.bcast(None, root=0)
                     ^^^^^^^^^^^^^^^^^^^^^^^^
  File "src/mpi4py/MPI.src/Comm.pyx", line 2113, in mpi4py.MPI.Comm.bcast
_pickle.UnpicklingError: invalid load key, '\x00'.
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 206, in mpi4py.MPI.pickle_load
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 776, in mpi4py.MPI.PyMPI_bcast
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 776, in mpi4py.MPI.PyMPI_bcast
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 776, in mpi4py.MPI.PyMPI_bcast
    global_weights = comm.bcast(None, root=0)
                     ^^^^^^^^^^^^^^^^^^^^^^^^
  File "src/mpi4py/MPI.src/Comm.pyx", line 2113, in mpi4py.MPI.Comm.bcast
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 195, in mpi4py.MPI.cloads
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 776, in mpi4py.MPI.PyMPI_bcast
_pickle.UnpicklingError: invalid load key, '\x00'.
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 206, in mpi4py.MPI.pickle_load
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 206, in mpi4py.MPI.pickle_load
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 206, in mpi4py.MPI.pickle_load
    global_weights = comm.bcast(None, root=0)
                     ^^^^^^^^^^^^^^^^^^^^^^^^
  File "src/mpi4py/MPI.src/Comm.pyx", line 2113, in mpi4py.MPI.Comm.bcast
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 206, in mpi4py.MPI.pickle_load
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 195, in mpi4py.MPI.cloads
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 206, in mpi4py.MPI.pickle_load
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 195, in mpi4py.MPI.cloads
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 195, in mpi4py.MPI.cloads
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 776, in mpi4py.MPI.PyMPI_bcast
_pickle.UnpicklingError: invalid load key, '\x00'.
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 195, in mpi4py.MPI.cloads
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 195, in mpi4py.MPI.cloads
_pickle.UnpicklingError: invalid load key, '\x00'.
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 206, in mpi4py.MPI.pickle_load
_pickle.UnpicklingError: invalid load key, '\x00'.
_pickle.UnpicklingError: invalid load key, '\x00'.
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 206, in mpi4py.MPI.pickle_load
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 206, in mpi4py.MPI.pickle_load
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 776, in mpi4py.MPI.PyMPI_bcast
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 195, in mpi4py.MPI.cloads
_pickle.UnpicklingError: invalid load key, '\x00'.
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 195, in mpi4py.MPI.cloads
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 776, in mpi4py.MPI.PyMPI_bcast
_pickle.UnpicklingError: invalid load key, '\x00'.
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 206, in mpi4py.MPI.pickle_load
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 195, in mpi4py.MPI.cloads
_pickle.UnpicklingError: invalid load key, '\x00'.
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 195, in mpi4py.MPI.cloads
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 776, in mpi4py.MPI.PyMPI_bcast
    global_weights = comm.bcast(None, root=0)
                     ^^^^^^^^^^^^^^^^^^^^^^^^
  File "src/mpi4py/MPI.src/Comm.pyx", line 2113, in mpi4py.MPI.Comm.bcast
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 195, in mpi4py.MPI.cloads
_pickle.UnpicklingError: invalid load key, '\x00'.
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 776, in mpi4py.MPI.PyMPI_bcast
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 206, in mpi4py.MPI.pickle_load
_pickle.UnpicklingError: invalid load key, '\x00'.
_pickle.UnpicklingError: invalid load key, '\x00'.
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 776, in mpi4py.MPI.PyMPI_bcast
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 206, in mpi4py.MPI.pickle_load
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 195, in mpi4py.MPI.cloads
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 195, in mpi4py.MPI.cloads
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 206, in mpi4py.MPI.pickle_load
_pickle.UnpicklingError: invalid load key, '\x00'.
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 206, in mpi4py.MPI.pickle_load
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 195, in mpi4py.MPI.cloads
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 206, in mpi4py.MPI.pickle_load
_pickle.UnpicklingError: invalid load key, '\x00'.
_pickle.UnpicklingError: invalid load key, '\x00'.
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 195, in mpi4py.MPI.cloads
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 195, in mpi4py.MPI.cloads
_pickle.UnpicklingError: invalid load key, '\x00'.
_pickle.UnpicklingError: invalid load key, '\x00'.
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_reset_if_to: cdr2344 [6]: pmixp_coll_tree.c:1321: 0x14561408c6a0: collective timeout seq=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_log: cdr2344 [6]: pmixp_coll.c:286: Dumping collective state
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2344 [6]: pmixp_coll_tree.c:1337: 0x14561408c6a0: COLL_FENCE_TREE state seq=1 contribs: loc=1/prnt=0/child=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2344 [6]: pmixp_coll_tree.c:1342: my peerid: 20:cdr1727
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2344 [6]: pmixp_coll_tree.c:1345: root host: 17:cdr1703
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2344 [6]: pmixp_coll_tree.c:1348: prnt host: 4:cdr2277
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2344 [6]: pmixp_coll_tree.c:1350: prnt contrib:
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2344 [6]: pmixp_coll_tree.c:1351: 	 [4:cdr2277] false
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2344 [6]: pmixp_coll_tree.c:1392: status: coll=COLL_UPFWD upfw=COLL_SND_DONE dfwd=COLL_SND_NONE
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2344 [6]: pmixp_coll_tree.c:1396: dfwd status: dfwd_cb_cnt=0, dfwd_cb_wait=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2344 [6]: pmixp_coll_tree.c:1398: bufs (offset/size): upfw 69/16415, dfwd 69/16415
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_reset_if_to: cdr2458 [16]: pmixp_coll_tree.c:1321: 0x1483280a2860: collective timeout seq=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_log: cdr2458 [16]: pmixp_coll.c:286: Dumping collective state
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2458 [16]: pmixp_coll_tree.c:1337: 0x1483280a2860: COLL_FENCE_TREE state seq=1 contribs: loc=1/prnt=0/child=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2458 [16]: pmixp_coll_tree.c:1342: my peerid: 30:cdr1763
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2458 [16]: pmixp_coll_tree.c:1345: root host: 17:cdr1703
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2458 [16]: pmixp_coll_tree.c:1348: prnt host: 4:cdr2277
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2458 [16]: pmixp_coll_tree.c:1350: prnt contrib:
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2458 [16]: pmixp_coll_tree.c:1351: 	 [4:cdr2277] false
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2458 [16]: pmixp_coll_tree.c:1392: status: coll=COLL_UPFWD upfw=COLL_SND_DONE dfwd=COLL_SND_NONE
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2458 [16]: pmixp_coll_tree.c:1396: dfwd status: dfwd_cb_cnt=0, dfwd_cb_wait=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2458 [16]: pmixp_coll_tree.c:1398: bufs (offset/size): upfw 69/16415, dfwd 69/16415
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_reset_if_to: cdr1729 [21]: pmixp_coll_tree.c:1321: 0x14fc70081340: collective timeout seq=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_log: cdr1729 [21]: pmixp_coll.c:286: Dumping collective state
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1729 [21]: pmixp_coll_tree.c:1337: 0x14fc70081340: COLL_FENCE_TREE state seq=1 contribs: loc=1/prnt=0/child=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1729 [21]: pmixp_coll_tree.c:1342: my peerid: 4:cdr2277
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1729 [21]: pmixp_coll_tree.c:1345: root host: 17:cdr1703
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1729 [21]: pmixp_coll_tree.c:1348: prnt host: 18:cdr1707
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1729 [21]: pmixp_coll_tree.c:1350: prnt contrib:
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1729 [21]: pmixp_coll_tree.c:1351: 	 [18:cdr1707] false
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1729 [21]: pmixp_coll_tree.c:1392: status: coll=COLL_UPFWD upfw=COLL_SND_DONE dfwd=COLL_SND_NONE
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1729 [21]: pmixp_coll_tree.c:1396: dfwd status: dfwd_cb_cnt=0, dfwd_cb_wait=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1729 [21]: pmixp_coll_tree.c:1398: bufs (offset/size): upfw 69/16415, dfwd 69/16415
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_reset_if_to: cdr2450 [14]: pmixp_coll_tree.c:1321: 0x14631c096f10: collective timeout seq=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_log: cdr2450 [14]: pmixp_coll.c:286: Dumping collective state
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2450 [14]: pmixp_coll_tree.c:1337: 0x14631c096f10: COLL_FENCE_TREE state seq=1 contribs: loc=1/prnt=0/child=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2450 [14]: pmixp_coll_tree.c:1342: my peerid: 28:cdr1760
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2450 [14]: pmixp_coll_tree.c:1345: root host: 17:cdr1703
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2450 [14]: pmixp_coll_tree.c:1348: prnt host: 4:cdr2277
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2450 [14]: pmixp_coll_tree.c:1350: prnt contrib:
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2450 [14]: pmixp_coll_tree.c:1351: 	 [4:cdr2277] false
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2450 [14]: pmixp_coll_tree.c:1392: status: coll=COLL_UPFWD upfw=COLL_SND_DONE dfwd=COLL_SND_NONE
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2450 [14]: pmixp_coll_tree.c:1396: dfwd status: dfwd_cb_cnt=0, dfwd_cb_wait=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2450 [14]: pmixp_coll_tree.c:1398: bufs (offset/size): upfw 69/16415, dfwd 69/16415
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_reset_if_to: cdr1717 [19]: pmixp_coll_tree.c:1321: 0x147ce4081440: collective timeout seq=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_log: cdr1717 [19]: pmixp_coll.c:286: Dumping collective state
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1717 [19]: pmixp_coll_tree.c:1337: 0x147ce4081440: COLL_FENCE_TREE state seq=1 contribs: loc=1/prnt=0/child=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1717 [19]: pmixp_coll_tree.c:1342: my peerid: 2:cdr2242
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1717 [19]: pmixp_coll_tree.c:1345: root host: 17:cdr1703
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1717 [19]: pmixp_coll_tree.c:1348: prnt host: 18:cdr1707
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1717 [19]: pmixp_coll_tree.c:1350: prnt contrib:
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1717 [19]: pmixp_coll_tree.c:1351: 	 [18:cdr1707] false
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1717 [19]: pmixp_coll_tree.c:1392: status: coll=COLL_UPFWD upfw=COLL_SND_DONE dfwd=COLL_SND_NONE
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1717 [19]: pmixp_coll_tree.c:1396: dfwd status: dfwd_cb_cnt=0, dfwd_cb_wait=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1717 [19]: pmixp_coll_tree.c:1398: bufs (offset/size): upfw 69/16415, dfwd 69/16415
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_reset_if_to: cdr1736 [24]: pmixp_coll_tree.c:1321: 0x14e6400a36f0: collective timeout seq=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_log: cdr1736 [24]: pmixp_coll.c:286: Dumping collective state
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1736 [24]: pmixp_coll_tree.c:1337: 0x14e6400a36f0: COLL_FENCE_TREE state seq=1 contribs: loc=1/prnt=0/child=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1736 [24]: pmixp_coll_tree.c:1342: my peerid: 7:cdr2357
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1736 [24]: pmixp_coll_tree.c:1345: root host: 17:cdr1703
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1736 [24]: pmixp_coll_tree.c:1348: prnt host: 18:cdr1707
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1736 [24]: pmixp_coll_tree.c:1350: prnt contrib:
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1736 [24]: pmixp_coll_tree.c:1351: 	 [18:cdr1707] false
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1736 [24]: pmixp_coll_tree.c:1392: status: coll=COLL_UPFWD upfw=COLL_SND_DONE dfwd=COLL_SND_NONE
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1736 [24]: pmixp_coll_tree.c:1396: dfwd status: dfwd_cb_cnt=0, dfwd_cb_wait=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1736 [24]: pmixp_coll_tree.c:1398: bufs (offset/size): upfw 69/16415, dfwd 69/16415
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_reset_if_to: cdr2432 [12]: pmixp_coll_tree.c:1321: 0x155180081e10: collective timeout seq=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_log: cdr2432 [12]: pmixp_coll.c:286: Dumping collective state
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2432 [12]: pmixp_coll_tree.c:1337: 0x155180081e10: COLL_FENCE_TREE state seq=1 contribs: loc=1/prnt=0/child=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2432 [12]: pmixp_coll_tree.c:1342: my peerid: 26:cdr1746
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2432 [12]: pmixp_coll_tree.c:1345: root host: 17:cdr1703
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2432 [12]: pmixp_coll_tree.c:1348: prnt host: 4:cdr2277
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2432 [12]: pmixp_coll_tree.c:1350: prnt contrib:
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2432 [12]: pmixp_coll_tree.c:1351: 	 [4:cdr2277] false
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2432 [12]: pmixp_coll_tree.c:1392: status: coll=COLL_UPFWD upfw=COLL_SND_DONE dfwd=COLL_SND_NONE
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2432 [12]: pmixp_coll_tree.c:1396: dfwd status: dfwd_cb_cnt=0, dfwd_cb_wait=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2432 [12]: pmixp_coll_tree.c:1398: bufs (offset/size): upfw 69/16415, dfwd 69/16415
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_reset_if_to: cdr1707 [18]: pmixp_coll_tree.c:1321: 0x151c2c0a18a0: collective timeout seq=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_log: cdr1707 [18]: pmixp_coll.c:286: Dumping collective state
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1707 [18]: pmixp_coll_tree.c:1337: 0x151c2c0a18a0: COLL_FENCE_TREE state seq=1 contribs: loc=1/prnt=0/child=11
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1707 [18]: pmixp_coll_tree.c:1342: my peerid: 1:cdr2160
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1707 [18]: pmixp_coll_tree.c:1345: root host: 17:cdr1703
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1707 [18]: pmixp_coll_tree.c:1348: prnt host: 17:cdr1703
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1707 [18]: pmixp_coll_tree.c:1350: prnt contrib:
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1707 [18]: pmixp_coll_tree.c:1351: 	 [17:cdr1703] false
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1707 [18]: pmixp_coll_tree.c:1359: child contribs [16]:
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1707 [18]: pmixp_coll_tree.c:1385: 	 done contrib: cdr[1717,1729,1732,1736,1746,1749,1760,2091,2160,2242,2255]
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1707 [18]: pmixp_coll_tree.c:1387: 	 wait contrib: cdr[1727,1735,1739,1762-1763]
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1707 [18]: pmixp_coll_tree.c:1392: status: coll=COLL_COLLECT upfw=COLL_SND_NONE dfwd=COLL_SND_NONE
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1707 [18]: pmixp_coll_tree.c:1396: dfwd status: dfwd_cb_cnt=0, dfwd_cb_wait=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1707 [18]: pmixp_coll_tree.c:1398: bufs (offset/size): upfw 69/16415, dfwd 69/16415
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_reset_if_to: cdr2277 [4]: pmixp_coll_tree.c:1321: 0x149f9c089a00: collective timeout seq=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_log: cdr2277 [4]: pmixp_coll.c:286: Dumping collective state
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2277 [4]: pmixp_coll_tree.c:1337: 0x149f9c089a00: COLL_FENCE_TREE state seq=1 contribs: loc=0/prnt=0/child=6
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2277 [4]: pmixp_coll_tree.c:1342: my peerid: 18:cdr1707
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2277 [4]: pmixp_coll_tree.c:1345: root host: 17:cdr1703
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2277 [4]: pmixp_coll_tree.c:1348: prnt host: 17:cdr1703
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2277 [4]: pmixp_coll_tree.c:1350: prnt contrib:
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2277 [4]: pmixp_coll_tree.c:1351: 	 [17:cdr1703] false
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2277 [4]: pmixp_coll_tree.c:1359: child contribs [12]:
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2277 [4]: pmixp_coll_tree.c:1385: 	 done contrib: cdr[2344,2412,2423,2432,2450,2458]
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2277 [4]: pmixp_coll_tree.c:1387: 	 wait contrib: cdr[2297,2357,2363,2407,2440,2453]
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2277 [4]: pmixp_coll_tree.c:1392: status: coll=COLL_COLLECT upfw=COLL_SND_NONE dfwd=COLL_SND_NONE
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2277 [4]: pmixp_coll_tree.c:1396: dfwd status: dfwd_cb_cnt=0, dfwd_cb_wait=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2277 [4]: pmixp_coll_tree.c:1398: bufs (offset/size): upfw 69/16415, dfwd 69/16415
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_reset_if_to: cdr2423 [11]: pmixp_coll_tree.c:1321: 0x147508082430: collective timeout seq=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_log: cdr2423 [11]: pmixp_coll.c:286: Dumping collective state
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2423 [11]: pmixp_coll_tree.c:1337: 0x147508082430: COLL_FENCE_TREE state seq=1 contribs: loc=1/prnt=0/child=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2423 [11]: pmixp_coll_tree.c:1342: my peerid: 25:cdr1739
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2423 [11]: pmixp_coll_tree.c:1345: root host: 17:cdr1703
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2423 [11]: pmixp_coll_tree.c:1348: prnt host: 4:cdr2277
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2423 [11]: pmixp_coll_tree.c:1350: prnt contrib:
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2423 [11]: pmixp_coll_tree.c:1351: 	 [4:cdr2277] false
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2423 [11]: pmixp_coll_tree.c:1392: status: coll=COLL_UPFWD upfw=COLL_SND_DONE dfwd=COLL_SND_NONE
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2423 [11]: pmixp_coll_tree.c:1396: dfwd status: dfwd_cb_cnt=0, dfwd_cb_wait=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2423 [11]: pmixp_coll_tree.c:1398: bufs (offset/size): upfw 69/16415, dfwd 69/16415
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_reset_if_to: cdr2242 [2]: pmixp_coll_tree.c:1321: 0x14bc6c086be0: collective timeout seq=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_log: cdr2242 [2]: pmixp_coll.c:286: Dumping collective state
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2242 [2]: pmixp_coll_tree.c:1337: 0x14bc6c086be0: COLL_FENCE_TREE state seq=1 contribs: loc=1/prnt=0/child=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2242 [2]: pmixp_coll_tree.c:1342: my peerid: 16:cdr2458
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2242 [2]: pmixp_coll_tree.c:1345: root host: 17:cdr1703
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2242 [2]: pmixp_coll_tree.c:1348: prnt host: 18:cdr1707
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2242 [2]: pmixp_coll_tree.c:1350: prnt contrib:
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2242 [2]: pmixp_coll_tree.c:1351: 	 [18:cdr1707] false
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2242 [2]: pmixp_coll_tree.c:1392: status: coll=COLL_UPFWD upfw=COLL_SND_DONE dfwd=COLL_SND_NONE
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2242 [2]: pmixp_coll_tree.c:1396: dfwd status: dfwd_cb_cnt=0, dfwd_cb_wait=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2242 [2]: pmixp_coll_tree.c:1398: bufs (offset/size): upfw 69/16415, dfwd 69/16415
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_reset_if_to: cdr2412 [10]: pmixp_coll_tree.c:1321: 0x14eeb4078420: collective timeout seq=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_log: cdr2412 [10]: pmixp_coll.c:286: Dumping collective state
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2412 [10]: pmixp_coll_tree.c:1337: 0x14eeb4078420: COLL_FENCE_TREE state seq=1 contribs: loc=1/prnt=0/child=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2412 [10]: pmixp_coll_tree.c:1342: my peerid: 24:cdr1736
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2412 [10]: pmixp_coll_tree.c:1345: root host: 17:cdr1703
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2412 [10]: pmixp_coll_tree.c:1348: prnt host: 4:cdr2277
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2412 [10]: pmixp_coll_tree.c:1350: prnt contrib:
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2412 [10]: pmixp_coll_tree.c:1351: 	 [4:cdr2277] false
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2412 [10]: pmixp_coll_tree.c:1392: status: coll=COLL_UPFWD upfw=COLL_SND_DONE dfwd=COLL_SND_NONE
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2412 [10]: pmixp_coll_tree.c:1396: dfwd status: dfwd_cb_cnt=0, dfwd_cb_wait=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2412 [10]: pmixp_coll_tree.c:1398: bufs (offset/size): upfw 69/16415, dfwd 69/16415
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_reset_if_to: cdr1732 [22]: pmixp_coll_tree.c:1321: 0x14e1440a3ac0: collective timeout seq=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_log: cdr1732 [22]: pmixp_coll.c:286: Dumping collective state
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1732 [22]: pmixp_coll_tree.c:1337: 0x14e1440a3ac0: COLL_FENCE_TREE state seq=1 contribs: loc=1/prnt=0/child=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1732 [22]: pmixp_coll_tree.c:1342: my peerid: 5:cdr2297
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1732 [22]: pmixp_coll_tree.c:1345: root host: 17:cdr1703
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1732 [22]: pmixp_coll_tree.c:1348: prnt host: 18:cdr1707
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1732 [22]: pmixp_coll_tree.c:1350: prnt contrib:
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1732 [22]: pmixp_coll_tree.c:1351: 	 [18:cdr1707] false
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1732 [22]: pmixp_coll_tree.c:1392: status: coll=COLL_UPFWD upfw=COLL_SND_DONE dfwd=COLL_SND_NONE
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1732 [22]: pmixp_coll_tree.c:1396: dfwd status: dfwd_cb_cnt=0, dfwd_cb_wait=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1732 [22]: pmixp_coll_tree.c:1398: bufs (offset/size): upfw 69/16415, dfwd 69/16415
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_reset_if_to: cdr1760 [28]: pmixp_coll_tree.c:1321: 0x152e100a7670: collective timeout seq=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_log: cdr1760 [28]: pmixp_coll.c:286: Dumping collective state
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1760 [28]: pmixp_coll_tree.c:1337: 0x152e100a7670: COLL_FENCE_TREE state seq=1 contribs: loc=1/prnt=0/child=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1760 [28]: pmixp_coll_tree.c:1342: my peerid: 11:cdr2423
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1760 [28]: pmixp_coll_tree.c:1345: root host: 17:cdr1703
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1760 [28]: pmixp_coll_tree.c:1348: prnt host: 18:cdr1707
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1760 [28]: pmixp_coll_tree.c:1350: prnt contrib:
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1760 [28]: pmixp_coll_tree.c:1351: 	 [18:cdr1707] false
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1760 [28]: pmixp_coll_tree.c:1392: status: coll=COLL_UPFWD upfw=COLL_SND_DONE dfwd=COLL_SND_NONE
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1760 [28]: pmixp_coll_tree.c:1396: dfwd status: dfwd_cb_cnt=0, dfwd_cb_wait=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1760 [28]: pmixp_coll_tree.c:1398: bufs (offset/size): upfw 69/16415, dfwd 69/16415
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_reset_if_to: cdr2160 [1]: pmixp_coll_tree.c:1321: 0x148fa40815d0: collective timeout seq=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_log: cdr2160 [1]: pmixp_coll.c:286: Dumping collective state
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2160 [1]: pmixp_coll_tree.c:1337: 0x148fa40815d0: COLL_FENCE_TREE state seq=1 contribs: loc=1/prnt=0/child=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2160 [1]: pmixp_coll_tree.c:1342: my peerid: 15:cdr2453
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2160 [1]: pmixp_coll_tree.c:1345: root host: 17:cdr1703
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2160 [1]: pmixp_coll_tree.c:1348: prnt host: 18:cdr1707
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2160 [1]: pmixp_coll_tree.c:1350: prnt contrib:
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2160 [1]: pmixp_coll_tree.c:1351: 	 [18:cdr1707] false
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2160 [1]: pmixp_coll_tree.c:1392: status: coll=COLL_UPFWD upfw=COLL_SND_DONE dfwd=COLL_SND_NONE
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2160 [1]: pmixp_coll_tree.c:1396: dfwd status: dfwd_cb_cnt=0, dfwd_cb_wait=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2160 [1]: pmixp_coll_tree.c:1398: bufs (offset/size): upfw 69/16415, dfwd 69/16415
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_reset_if_to: cdr2255 [3]: pmixp_coll_tree.c:1321: 0x1486ec0814f0: collective timeout seq=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_log: cdr2255 [3]: pmixp_coll.c:286: Dumping collective state
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2255 [3]: pmixp_coll_tree.c:1337: 0x1486ec0814f0: COLL_FENCE_TREE state seq=1 contribs: loc=1/prnt=0/child=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2255 [3]: pmixp_coll_tree.c:1342: my peerid: 17:cdr1703
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2255 [3]: pmixp_coll_tree.c:1345: root host: 17:cdr1703
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2255 [3]: pmixp_coll_tree.c:1348: prnt host: 18:cdr1707
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2255 [3]: pmixp_coll_tree.c:1350: prnt contrib:
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2255 [3]: pmixp_coll_tree.c:1351: 	 [18:cdr1707] false
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2255 [3]: pmixp_coll_tree.c:1392: status: coll=COLL_UPFWD upfw=COLL_SND_DONE dfwd=COLL_SND_NONE
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2255 [3]: pmixp_coll_tree.c:1396: dfwd status: dfwd_cb_cnt=0, dfwd_cb_wait=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2255 [3]: pmixp_coll_tree.c:1398: bufs (offset/size): upfw 69/16415, dfwd 69/16415
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_reset_if_to: cdr1749 [27]: pmixp_coll_tree.c:1321: 0x14ceac08d9a0: collective timeout seq=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_log: cdr1749 [27]: pmixp_coll.c:286: Dumping collective state
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1749 [27]: pmixp_coll_tree.c:1337: 0x14ceac08d9a0: COLL_FENCE_TREE state seq=1 contribs: loc=1/prnt=0/child=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1749 [27]: pmixp_coll_tree.c:1342: my peerid: 10:cdr2412
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1749 [27]: pmixp_coll_tree.c:1345: root host: 17:cdr1703
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1749 [27]: pmixp_coll_tree.c:1348: prnt host: 18:cdr1707
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1749 [27]: pmixp_coll_tree.c:1350: prnt contrib:
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1749 [27]: pmixp_coll_tree.c:1351: 	 [18:cdr1707] false
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1749 [27]: pmixp_coll_tree.c:1392: status: coll=COLL_UPFWD upfw=COLL_SND_DONE dfwd=COLL_SND_NONE
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1749 [27]: pmixp_coll_tree.c:1396: dfwd status: dfwd_cb_cnt=0, dfwd_cb_wait=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1749 [27]: pmixp_coll_tree.c:1398: bufs (offset/size): upfw 69/16415, dfwd 69/16415
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_reset_if_to: cdr1746 [26]: pmixp_coll_tree.c:1321: 0x14e9040a5510: collective timeout seq=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_log: cdr1746 [26]: pmixp_coll.c:286: Dumping collective state
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1746 [26]: pmixp_coll_tree.c:1337: 0x14e9040a5510: COLL_FENCE_TREE state seq=1 contribs: loc=1/prnt=0/child=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1746 [26]: pmixp_coll_tree.c:1342: my peerid: 9:cdr2407
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1746 [26]: pmixp_coll_tree.c:1345: root host: 17:cdr1703
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1746 [26]: pmixp_coll_tree.c:1348: prnt host: 18:cdr1707
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1746 [26]: pmixp_coll_tree.c:1350: prnt contrib:
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1746 [26]: pmixp_coll_tree.c:1351: 	 [18:cdr1707] false
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1746 [26]: pmixp_coll_tree.c:1392: status: coll=COLL_UPFWD upfw=COLL_SND_DONE dfwd=COLL_SND_NONE
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1746 [26]: pmixp_coll_tree.c:1396: dfwd status: dfwd_cb_cnt=0, dfwd_cb_wait=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1746 [26]: pmixp_coll_tree.c:1398: bufs (offset/size): upfw 69/16415, dfwd 69/16415
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_reset_if_to: cdr2091 [0]: pmixp_coll_tree.c:1321: 0x14edbc0790f0: collective timeout seq=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_log: cdr2091 [0]: pmixp_coll.c:286: Dumping collective state
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2091 [0]: pmixp_coll_tree.c:1337: 0x14edbc0790f0: COLL_FENCE_TREE state seq=1 contribs: loc=1/prnt=0/child=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2091 [0]: pmixp_coll_tree.c:1342: my peerid: 14:cdr2450
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2091 [0]: pmixp_coll_tree.c:1345: root host: 17:cdr1703
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2091 [0]: pmixp_coll_tree.c:1348: prnt host: 18:cdr1707
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2091 [0]: pmixp_coll_tree.c:1350: prnt contrib:
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2091 [0]: pmixp_coll_tree.c:1351: 	 [18:cdr1707] false
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2091 [0]: pmixp_coll_tree.c:1392: status: coll=COLL_UPFWD upfw=COLL_SND_DONE dfwd=COLL_SND_NONE
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2091 [0]: pmixp_coll_tree.c:1396: dfwd status: dfwd_cb_cnt=0, dfwd_cb_wait=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2091 [0]: pmixp_coll_tree.c:1398: bufs (offset/size): upfw 69/16415, dfwd 69/16415
srun: error: cdr2242: task 2: Exited with exit code 1
srun: Terminating StepId=64636422.0
slurmstepd: error: *** STEP 64636422.0 ON cdr2091 CANCELLED AT 2025-07-24T08:54:09 ***
srun: error: cdr2344: task 6: Exited with exit code 1
srun: error: cdr1717: task 19: Exited with exit code 1
srun: error: cdr2458: task 16: Exited with exit code 1
srun: error: cdr1707: task 18: Exited with exit code 1
srun: error: cdr2432: task 12: Exited with exit code 1
srun: error: cdr1729: task 21: Exited with exit code 1
srun: error: cdr2423: task 11: Exited with exit code 1
srun: error: cdr1736: task 24: Exited with exit code 1
srun: error: cdr1732: task 22: Terminated
srun: error: cdr2450: task 14: Exited with exit code 1
srun: error: cdr2091: task 0: Terminated
srun: error: cdr1749: task 27: Terminated
srun: error: cdr1703: task 17: Terminated
srun: error: cdr2407: task 9: Terminated
srun: error: cdr1760: task 28: Terminated
srun: error: cdr2412: task 10: Terminated
srun: error: cdr2440: task 13: Terminated
srun: error: cdr1735: task 23: Terminated
srun: error: cdr2277: task 4: Terminated
srun: error: cdr2255: task 3: Terminated
srun: error: cdr1727: task 20: Terminated
srun: error: cdr1763: task 30: Terminated
srun: error: cdr2363: task 8: Terminated
srun: error: cdr2160: task 1: Terminated
srun: error: cdr2357: task 7: Terminated
srun: error: cdr1746: task 26: Terminated
srun: error: cdr2453: task 15: Terminated
srun: error: cdr1739: task 25: Terminated
srun: error: cdr2297: task 5: Terminated
srun: error: cdr1762: task 29: Terminated
srun: Force Terminated StepId=64636422.0
cp: cannot stat 'federated_metrics.png': No such file or directory
2025-07-24 08:54:26.494706: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:54:26.516013: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:54:26.523068: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:54:26.540877: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:54:26.543784: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:54:26.548378: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:54:26.566783: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:54:26.567397: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:54:26.574622: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:54:26.590007: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:54:26.592473: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:54:26.611079: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:54:26.635698: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:54:26.643179: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:54:26.654430: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:54:26.661392: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:54:26.675590: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:54:26.700166: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:54:26.707604: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:54:26.725936: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:54:26.851920: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:54:26.867805: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:54:26.872794: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:54:26.878867: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:54:26.890378: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:54:26.896797: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:54:26.900631: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:54:26.904076: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:54:26.920915: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:54:26.922152: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:54:26.928649: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:54:26.930481: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:54:26.938060: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:54:26.948843: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:54:26.957696: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:54:27.036313: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:54:27.049554: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:54:27.056933: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:54:27.066762: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:54:27.070137: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:54:27.080682: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:54:27.087812: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:54:27.087903: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:54:27.093594: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:54:27.100780: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:54:27.105737: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:54:27.112388: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:54:27.118506: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:54:27.119725: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:54:27.127302: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:54:27.138037: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:54:27.148604: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:54:27.162908: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:54:27.173510: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:54:27.181024: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:54:27.183960: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:54:27.187066: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:54:27.199568: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:54:27.201631: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:54:27.208595: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:54:27.210372: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:54:27.210728: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:54:27.213072: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:54:27.215949: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:54:27.223638: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:54:27.232899: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:54:27.232854: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:54:27.234156: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:54:27.234291: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:54:27.236668: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:54:27.244301: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:54:27.249294: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:54:27.253747: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:54:27.257019: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:54:27.258385: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:54:27.258492: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:54:27.264186: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:54:27.264444: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:54:27.265863: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:54:27.265955: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:54:27.275738: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:54:27.277851: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:54:27.284087: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:54:27.284993: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:54:27.285147: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:54:27.287763: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:54:27.303210: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:54:27.314079: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:54:27.321677: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:54:27.341232: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:54:27.473345: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:54:27.497777: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:54:27.525218: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:54:27.533103: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:54:27.554262: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:54:27.747029: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:54:27.771802: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:54:27.797751: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:54:27.805224: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:54:27.824850: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:54:27.832814: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:54:27.855229: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:54:27.880801: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:54:27.888315: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:54:27.907556: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:54:29.063122: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:54:29.084308: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:54:29.108547: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:54:29.115853: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:54:29.134088: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:54:29.161248: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:54:29.182995: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:54:29.208466: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:54:29.216036: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:54:29.237997: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:54:29.401998: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:54:29.417725: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:54:29.423470: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:54:29.439348: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:54:29.448337: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:54:29.455670: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:54:29.462449: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:54:29.463865: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:54:29.471225: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:54:29.474034: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:54:29.485410: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:54:29.489576: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:54:29.511710: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:54:29.519286: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:54:29.539051: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:54:29.632298: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:54:29.653180: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:54:29.677482: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:54:29.684763: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:54:29.702838: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:54:30.003696: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:54:30.028554: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:54:30.055213: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:54:30.062671: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:54:30.082318: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:54:30.449458: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:54:30.475742: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:54:30.476722: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:54:30.500652: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:54:30.504792: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:54:30.513138: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:54:30.526454: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:54:30.534027: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:54:30.536440: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:54:30.554188: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:54:41.471780: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:54:42.024179: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:54:42.233870: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:54:43.336166: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:54:43.504161: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:54:43.630235: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:54:43.736117: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:54:44.145028: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:54:44.282069: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:54:44.563288: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:54:44.631066: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:54:45.300940: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:54:45.617787: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:54:45.763984: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:54:45.784974: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:54:46.405972: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:54:48.089257: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:54:48.605561: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:54:48.782096: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:54:49.329419: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:54:49.458468: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:54:50.951616: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:54:53.297240: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-24 08:54:53.319920: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-24 08:54:53.345132: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-24 08:54:53.352773: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-24 08:54:53.372111: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-24 08:55:01.053557: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:55:01.867810: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:55:02.904402: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:55:03.808343: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:55:03.832386: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:55:06.467589: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:55:07.616000: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-24 08:55:08.925474: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
2025-07-24 08:56:20.849298: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/home/aymenoml/venvs/fedcifar/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
Traceback (most recent call last):
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 15, in <module>
Traceback (most recent call last):
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 15, in <module>
Traceback (most recent call last):
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 15, in <module>
Traceback (most recent call last):
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 15, in <module>
Traceback (most recent call last):
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 15, in <module>
Traceback (most recent call last):
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 15, in <module>
Traceback (most recent call last):
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 15, in <module>
Traceback (most recent call last):
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 15, in <module>
Traceback (most recent call last):
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 15, in <module>
    main()
Traceback (most recent call last):
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 15, in <module>
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 12, in main
Traceback (most recent call last):
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 15, in <module>
Traceback (most recent call last):
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 15, in <module>
Traceback (most recent call last):
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 15, in <module>
    main()
    main()
    main()
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 12, in main
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 12, in main
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 12, in main
Traceback (most recent call last):
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 15, in <module>
    run_client(comm, rank)
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_client.py", line 69, in run_client
Traceback (most recent call last):
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 15, in <module>
    main()
Traceback (most recent call last):
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 15, in <module>
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 12, in main
    main()
    main()
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 12, in main
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 12, in main
    main()
    main()
Traceback (most recent call last):
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 15, in <module>
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 12, in main
    main()
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 12, in main
    run_client(comm, rank)
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 12, in main
    run_client(comm, rank)
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_client.py", line 69, in run_client
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_client.py", line 69, in run_client
    run_client(comm, rank)
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_client.py", line 69, in run_client
    run_client(comm, rank)
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_client.py", line 69, in run_client
    main()
Traceback (most recent call last):
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 12, in main
    run_client(comm, rank)
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 15, in <module>
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_client.py", line 69, in run_client
    run_client(comm, rank)
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_client.py", line 69, in run_client
Traceback (most recent call last):
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 15, in <module>
Traceback (most recent call last):
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 15, in <module>
    main()
    run_client(comm, rank)
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 12, in main
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_client.py", line 69, in run_client
    main()
    global_weights = comm.bcast(None, root=0)
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 12, in main
                     ^^^^^^^^^^^^^^^^^^^^^^^^
  File "src/mpi4py/MPI.src/Comm.pyx", line 2113, in mpi4py.MPI.Comm.bcast
    main()
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 12, in main
    main()
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 12, in main
    main()
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 12, in main
    run_client(comm, rank)
    run_client(comm, rank)
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_client.py", line 69, in run_client
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_client.py", line 69, in run_client
    run_client(comm, rank)
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_client.py", line 69, in run_client
    main()
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 12, in main
    run_client(comm, rank)
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_client.py", line 69, in run_client
    run_client(comm, rank)
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_client.py", line 69, in run_client
    run_client(comm, rank)
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_client.py", line 69, in run_client
    run_client(comm, rank)
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_client.py", line 69, in run_client
    global_weights = comm.bcast(None, root=0)
    global_weights = comm.bcast(None, root=0)
                     ^^^^^^^^^^^^^^^^^^^^^^^^
  File "src/mpi4py/MPI.src/Comm.pyx", line 2113, in mpi4py.MPI.Comm.bcast
                     ^^^^^^^^^^^^^^^^^^^^^^^^
  File "src/mpi4py/MPI.src/Comm.pyx", line 2113, in mpi4py.MPI.Comm.bcast
    global_weights = comm.bcast(None, root=0)
                     ^^^^^^^^^^^^^^^^^^^^^^^^
  File "src/mpi4py/MPI.src/Comm.pyx", line 2113, in mpi4py.MPI.Comm.bcast
    global_weights = comm.bcast(None, root=0)
                     ^^^^^^^^^^^^^^^^^^^^^^^^
  File "src/mpi4py/MPI.src/Comm.pyx", line 2113, in mpi4py.MPI.Comm.bcast
    global_weights = comm.bcast(None, root=0)
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 776, in mpi4py.MPI.PyMPI_bcast
                     ^^^^^^^^^^^^^^^^^^^^^^^^
  File "src/mpi4py/MPI.src/Comm.pyx", line 2113, in mpi4py.MPI.Comm.bcast
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 776, in mpi4py.MPI.PyMPI_bcast
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 206, in mpi4py.MPI.pickle_load
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 776, in mpi4py.MPI.PyMPI_bcast
    global_weights = comm.bcast(None, root=0)
                     ^^^^^^^^^^^^^^^^^^^^^^^^
  File "src/mpi4py/MPI.src/Comm.pyx", line 2113, in mpi4py.MPI.Comm.bcast
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 206, in mpi4py.MPI.pickle_load
    main()
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 12, in main
    main()
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 12, in main
    run_client(comm, rank)
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_client.py", line 69, in run_client
    run_client(comm, rank)
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_client.py", line 69, in run_client
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 195, in mpi4py.MPI.cloads
    global_weights = comm.bcast(None, root=0)
                     ^^^^^^^^^^^^^^^^^^^^^^^^
  File "src/mpi4py/MPI.src/Comm.pyx", line 2113, in mpi4py.MPI.Comm.bcast
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 206, in mpi4py.MPI.pickle_load
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 195, in mpi4py.MPI.cloads
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 776, in mpi4py.MPI.PyMPI_bcast
    global_weights = comm.bcast(None, root=0)
                     ^^^^^^^^^^^^^^^^^^^^^^^^
  File "src/mpi4py/MPI.src/Comm.pyx", line 2113, in mpi4py.MPI.Comm.bcast
    global_weights = comm.bcast(None, root=0)
                     ^^^^^^^^^^^^^^^^^^^^^^^^
  File "src/mpi4py/MPI.src/Comm.pyx", line 2113, in mpi4py.MPI.Comm.bcast
    main()
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_main.py", line 9, in main
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 776, in mpi4py.MPI.PyMPI_bcast
    global_weights = comm.bcast(None, root=0)
                     ^^^^^^^^^^^^^^^^^^^^^^^^
  File "src/mpi4py/MPI.src/Comm.pyx", line 2113, in mpi4py.MPI.Comm.bcast
    global_weights = comm.bcast(None, root=0)
    run_server(comm)
                     ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_server.py", line 46, in run_server
    run_client(comm, rank)
  File "src/mpi4py/MPI.src/Comm.pyx", line 2113, in mpi4py.MPI.Comm.bcast
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_client.py", line 69, in run_client
_pickle.UnpicklingError: invalid load key, '\x00'.
    global_weights = comm.bcast(None, root=0)
                     ^^^^^^^^^^^^^^^^^^^^^^^^
  File "src/mpi4py/MPI.src/Comm.pyx", line 2113, in mpi4py.MPI.Comm.bcast
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 206, in mpi4py.MPI.pickle_load
    global_weights = comm.bcast(None, root=0)
                     ^^^^^^^^^^^^^^^^^^^^^^^^
  File "src/mpi4py/MPI.src/Comm.pyx", line 2113, in mpi4py.MPI.Comm.bcast
    global_weights = comm.bcast(None, root=0)
                     ^^^^^^^^^^^^^^^^^^^^^^^^
  File "src/mpi4py/MPI.src/Comm.pyx", line 2113, in mpi4py.MPI.Comm.bcast
    comm.bcast(global_weights, root=0)
  File "src/mpi4py/MPI.src/Comm.pyx", line 2113, in mpi4py.MPI.Comm.bcast
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 776, in mpi4py.MPI.PyMPI_bcast
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 776, in mpi4py.MPI.PyMPI_bcast
    run_client(comm, rank)
  File "/scratch/aymenoml/system-utility-cifar10/federated_mpi/mpi_client.py", line 69, in run_client
    global_weights = comm.bcast(None, root=0)
                     ^^^^^^^^^^^^^^^^^^^^^^^^
  File "src/mpi4py/MPI.src/Comm.pyx", line 2113, in mpi4py.MPI.Comm.bcast
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 776, in mpi4py.MPI.PyMPI_bcast
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 776, in mpi4py.MPI.PyMPI_bcast
_pickle.UnpicklingError: invalid load key, '\x00'.
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 195, in mpi4py.MPI.cloads
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 206, in mpi4py.MPI.pickle_load
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 776, in mpi4py.MPI.PyMPI_bcast
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 776, in mpi4py.MPI.PyMPI_bcast
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 776, in mpi4py.MPI.PyMPI_bcast
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 195, in mpi4py.MPI.cloads
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 206, in mpi4py.MPI.pickle_load
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 206, in mpi4py.MPI.pickle_load
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 776, in mpi4py.MPI.PyMPI_bcast
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 195, in mpi4py.MPI.cloads
    global_weights = comm.bcast(None, root=0)
                     ^^^^^^^^^^^^^^^^^^^^^^^^
  File "src/mpi4py/MPI.src/Comm.pyx", line 2113, in mpi4py.MPI.Comm.bcast
_pickle.UnpicklingError: invalid load key, '\x00'.
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 195, in mpi4py.MPI.cloads
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 206, in mpi4py.MPI.pickle_load
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 776, in mpi4py.MPI.PyMPI_bcast
_pickle.UnpicklingError: invalid load key, '\x00'.
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 206, in mpi4py.MPI.pickle_load
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 206, in mpi4py.MPI.pickle_load
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 195, in mpi4py.MPI.cloads
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 206, in mpi4py.MPI.pickle_load
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 195, in mpi4py.MPI.cloads
    global_weights = comm.bcast(None, root=0)
                     ^^^^^^^^^^^^^^^^^^^^^^^^
  File "src/mpi4py/MPI.src/Comm.pyx", line 2113, in mpi4py.MPI.Comm.bcast
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 206, in mpi4py.MPI.pickle_load
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 195, in mpi4py.MPI.cloads
_pickle.UnpicklingError: invalid load key, '\x00'.
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 195, in mpi4py.MPI.cloads
    global_weights = comm.bcast(None, root=0)
                     ^^^^^^^^^^^^^^^^^^^^^^^^
  File "src/mpi4py/MPI.src/Comm.pyx", line 2113, in mpi4py.MPI.Comm.bcast
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 206, in mpi4py.MPI.pickle_load
_pickle.UnpicklingError: invalid load key, '\x00'.
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 195, in mpi4py.MPI.cloads
_pickle.UnpicklingError: invalid load key, '\x00'.
_pickle.UnpicklingError: invalid load key, '\x00'.
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 776, in mpi4py.MPI.PyMPI_bcast
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 195, in mpi4py.MPI.cloads
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 206, in mpi4py.MPI.pickle_load
_pickle.UnpicklingError: invalid load key, '\x00'.
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 776, in mpi4py.MPI.PyMPI_bcast
_pickle.UnpicklingError: invalid load key, '\x00'.
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 195, in mpi4py.MPI.cloads
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 776, in mpi4py.MPI.PyMPI_bcast
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 206, in mpi4py.MPI.pickle_load
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 776, in mpi4py.MPI.PyMPI_bcast
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 195, in mpi4py.MPI.cloads
_pickle.UnpicklingError: invalid load key, '\x00'.
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 776, in mpi4py.MPI.PyMPI_bcast
_pickle.UnpicklingError: invalid load key, '\x00'.
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 195, in mpi4py.MPI.cloads
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 206, in mpi4py.MPI.pickle_load
_pickle.UnpicklingError: invalid load key, '\x00'.
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 776, in mpi4py.MPI.PyMPI_bcast
_pickle.UnpicklingError: invalid load key, '\x00'.
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 195, in mpi4py.MPI.cloads
_pickle.UnpicklingError: invalid load key, '\x00'.
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 206, in mpi4py.MPI.pickle_load
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 206, in mpi4py.MPI.pickle_load
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 195, in mpi4py.MPI.cloads
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 195, in mpi4py.MPI.cloads
_pickle.UnpicklingError: invalid load key, '\x00'.
_pickle.UnpicklingError: invalid load key, '\x00'.
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 206, in mpi4py.MPI.pickle_load
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 195, in mpi4py.MPI.cloads
_pickle.UnpicklingError: invalid load key, '\x00'.
_pickle.UnpicklingError: invalid load key, '\x00'.
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 206, in mpi4py.MPI.pickle_load
  File "src/mpi4py/MPI.src/msgpickle.pxi", line 195, in mpi4py.MPI.cloads
_pickle.UnpicklingError: invalid load key, '\x00'.
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_reset_if_to: cdr1732 [22]: pmixp_coll_tree.c:1321: 0x1486cc09ad90: collective timeout seq=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_log: cdr1732 [22]: pmixp_coll.c:286: Dumping collective state
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1732 [22]: pmixp_coll_tree.c:1337: 0x1486cc09ad90: COLL_FENCE_TREE state seq=1 contribs: loc=1/prnt=0/child=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1732 [22]: pmixp_coll_tree.c:1342: my peerid: 5:cdr2297
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1732 [22]: pmixp_coll_tree.c:1345: root host: 17:cdr1703
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1732 [22]: pmixp_coll_tree.c:1348: prnt host: 18:cdr1707
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1732 [22]: pmixp_coll_tree.c:1350: prnt contrib:
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1732 [22]: pmixp_coll_tree.c:1351: 	 [18:cdr1707] false
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1732 [22]: pmixp_coll_tree.c:1392: status: coll=COLL_UPFWD upfw=COLL_SND_DONE dfwd=COLL_SND_NONE
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1732 [22]: pmixp_coll_tree.c:1396: dfwd status: dfwd_cb_cnt=0, dfwd_cb_wait=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1732 [22]: pmixp_coll_tree.c:1398: bufs (offset/size): upfw 69/16415, dfwd 69/16415
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_reset_if_to: cdr1760 [28]: pmixp_coll_tree.c:1321: 0x14d3f407ba20: collective timeout seq=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_log: cdr1760 [28]: pmixp_coll.c:286: Dumping collective state
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1760 [28]: pmixp_coll_tree.c:1337: 0x14d3f407ba20: COLL_FENCE_TREE state seq=1 contribs: loc=1/prnt=0/child=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1760 [28]: pmixp_coll_tree.c:1342: my peerid: 11:cdr2423
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1760 [28]: pmixp_coll_tree.c:1345: root host: 17:cdr1703
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1760 [28]: pmixp_coll_tree.c:1348: prnt host: 18:cdr1707
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1760 [28]: pmixp_coll_tree.c:1350: prnt contrib:
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1760 [28]: pmixp_coll_tree.c:1351: 	 [18:cdr1707] false
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1760 [28]: pmixp_coll_tree.c:1392: status: coll=COLL_UPFWD upfw=COLL_SND_DONE dfwd=COLL_SND_NONE
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1760 [28]: pmixp_coll_tree.c:1396: dfwd status: dfwd_cb_cnt=0, dfwd_cb_wait=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1760 [28]: pmixp_coll_tree.c:1398: bufs (offset/size): upfw 69/16415, dfwd 69/16415
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_reset_if_to: cdr2440 [13]: pmixp_coll_tree.c:1321: 0x14b0d0085520: collective timeout seq=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_log: cdr2440 [13]: pmixp_coll.c:286: Dumping collective state
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2440 [13]: pmixp_coll_tree.c:1337: 0x14b0d0085520: COLL_FENCE_TREE state seq=1 contribs: loc=1/prnt=0/child=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2440 [13]: pmixp_coll_tree.c:1342: my peerid: 27:cdr1749
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2440 [13]: pmixp_coll_tree.c:1345: root host: 17:cdr1703
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2440 [13]: pmixp_coll_tree.c:1348: prnt host: 4:cdr2277
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2440 [13]: pmixp_coll_tree.c:1350: prnt contrib:
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2440 [13]: pmixp_coll_tree.c:1351: 	 [4:cdr2277] false
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2440 [13]: pmixp_coll_tree.c:1392: status: coll=COLL_UPFWD upfw=COLL_SND_DONE dfwd=COLL_SND_NONE
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2440 [13]: pmixp_coll_tree.c:1396: dfwd status: dfwd_cb_cnt=0, dfwd_cb_wait=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2440 [13]: pmixp_coll_tree.c:1398: bufs (offset/size): upfw 69/16415, dfwd 69/16415
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_reset_if_to: cdr1703 [17]: pmixp_coll_tree.c:1321: 0x14ec08082a70: collective timeout seq=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_log: cdr1703 [17]: pmixp_coll.c:286: Dumping collective state
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1703 [17]: pmixp_coll_tree.c:1337: 0x14ec08082a70: COLL_FENCE_TREE state seq=1 contribs: loc=1/prnt=0/child=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1703 [17]: pmixp_coll_tree.c:1342: my peerid: 0:cdr2091
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1703 [17]: pmixp_coll_tree.c:1345: root host: 0:cdr2091
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1703 [17]: pmixp_coll_tree.c:1359: child contribs [2]:
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1703 [17]: pmixp_coll_tree.c:1385: 	 done contrib: -
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1703 [17]: pmixp_coll_tree.c:1387: 	 wait contrib: cdr[1707,2277]
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1703 [17]: pmixp_coll_tree.c:1392: status: coll=COLL_COLLECT upfw=COLL_SND_NONE dfwd=COLL_SND_NONE
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1703 [17]: pmixp_coll_tree.c:1396: dfwd status: dfwd_cb_cnt=0, dfwd_cb_wait=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1703 [17]: pmixp_coll_tree.c:1398: bufs (offset/size): upfw 69/16415, dfwd 69/16415
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_reset_if_to: cdr1707 [18]: pmixp_coll_tree.c:1321: 0x148a38009940: collective timeout seq=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_log: cdr1707 [18]: pmixp_coll.c:286: Dumping collective state
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1707 [18]: pmixp_coll_tree.c:1337: 0x148a38009940: COLL_FENCE_TREE state seq=1 contribs: loc=1/prnt=0/child=9
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1707 [18]: pmixp_coll_tree.c:1342: my peerid: 1:cdr2160
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1707 [18]: pmixp_coll_tree.c:1345: root host: 17:cdr1703
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1707 [18]: pmixp_coll_tree.c:1348: prnt host: 17:cdr1703
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1707 [18]: pmixp_coll_tree.c:1350: prnt contrib:
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1707 [18]: pmixp_coll_tree.c:1351: 	 [17:cdr1703] false
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1707 [18]: pmixp_coll_tree.c:1359: child contribs [16]:
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1707 [18]: pmixp_coll_tree.c:1385: 	 done contrib: cdr[1729,1732,1739,1746,1760,1763,2091,2242,2255]
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1707 [18]: pmixp_coll_tree.c:1387: 	 wait contrib: cdr[1717,1727,1735-1736,1749,1762,2160]
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1707 [18]: pmixp_coll_tree.c:1392: status: coll=COLL_COLLECT upfw=COLL_SND_NONE dfwd=COLL_SND_NONE
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1707 [18]: pmixp_coll_tree.c:1396: dfwd status: dfwd_cb_cnt=0, dfwd_cb_wait=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1707 [18]: pmixp_coll_tree.c:1398: bufs (offset/size): upfw 69/16415, dfwd 69/16415
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_reset_if_to: cdr2344 [6]: pmixp_coll_tree.c:1321: 0x1521f008c740: collective timeout seq=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_log: cdr2344 [6]: pmixp_coll.c:286: Dumping collective state
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2344 [6]: pmixp_coll_tree.c:1337: 0x1521f008c740: COLL_FENCE_TREE state seq=1 contribs: loc=1/prnt=0/child=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2344 [6]: pmixp_coll_tree.c:1342: my peerid: 20:cdr1727
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2344 [6]: pmixp_coll_tree.c:1345: root host: 17:cdr1703
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2344 [6]: pmixp_coll_tree.c:1348: prnt host: 4:cdr2277
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2344 [6]: pmixp_coll_tree.c:1350: prnt contrib:
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2344 [6]: pmixp_coll_tree.c:1351: 	 [4:cdr2277] false
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2344 [6]: pmixp_coll_tree.c:1392: status: coll=COLL_UPFWD upfw=COLL_SND_DONE dfwd=COLL_SND_NONE
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2344 [6]: pmixp_coll_tree.c:1396: dfwd status: dfwd_cb_cnt=0, dfwd_cb_wait=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2344 [6]: pmixp_coll_tree.c:1398: bufs (offset/size): upfw 69/16415, dfwd 69/16415
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_reset_if_to: cdr2407 [9]: pmixp_coll_tree.c:1321: 0x1510440913d0: collective timeout seq=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_log: cdr2407 [9]: pmixp_coll.c:286: Dumping collective state
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2407 [9]: pmixp_coll_tree.c:1337: 0x1510440913d0: COLL_FENCE_TREE state seq=1 contribs: loc=1/prnt=0/child=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2407 [9]: pmixp_coll_tree.c:1342: my peerid: 23:cdr1735
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2407 [9]: pmixp_coll_tree.c:1345: root host: 17:cdr1703
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2407 [9]: pmixp_coll_tree.c:1348: prnt host: 4:cdr2277
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2407 [9]: pmixp_coll_tree.c:1350: prnt contrib:
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2407 [9]: pmixp_coll_tree.c:1351: 	 [4:cdr2277] false
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2407 [9]: pmixp_coll_tree.c:1392: status: coll=COLL_UPFWD upfw=COLL_SND_DONE dfwd=COLL_SND_NONE
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2407 [9]: pmixp_coll_tree.c:1396: dfwd status: dfwd_cb_cnt=0, dfwd_cb_wait=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2407 [9]: pmixp_coll_tree.c:1398: bufs (offset/size): upfw 69/16415, dfwd 69/16415
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_reset_if_to: cdr2450 [14]: pmixp_coll_tree.c:1321: 0x148410078740: collective timeout seq=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_log: cdr2450 [14]: pmixp_coll.c:286: Dumping collective state
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2450 [14]: pmixp_coll_tree.c:1337: 0x148410078740: COLL_FENCE_TREE state seq=1 contribs: loc=1/prnt=0/child=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2450 [14]: pmixp_coll_tree.c:1342: my peerid: 28:cdr1760
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2450 [14]: pmixp_coll_tree.c:1345: root host: 17:cdr1703
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2450 [14]: pmixp_coll_tree.c:1348: prnt host: 4:cdr2277
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2450 [14]: pmixp_coll_tree.c:1350: prnt contrib:
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2450 [14]: pmixp_coll_tree.c:1351: 	 [4:cdr2277] false
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2450 [14]: pmixp_coll_tree.c:1392: status: coll=COLL_UPFWD upfw=COLL_SND_DONE dfwd=COLL_SND_NONE
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2450 [14]: pmixp_coll_tree.c:1396: dfwd status: dfwd_cb_cnt=0, dfwd_cb_wait=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2450 [14]: pmixp_coll_tree.c:1398: bufs (offset/size): upfw 69/16415, dfwd 69/16415
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_reset_if_to: cdr2255 [3]: pmixp_coll_tree.c:1321: 0x1510c8081480: collective timeout seq=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_log: cdr2255 [3]: pmixp_coll.c:286: Dumping collective state
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2255 [3]: pmixp_coll_tree.c:1337: 0x1510c8081480: COLL_FENCE_TREE state seq=1 contribs: loc=1/prnt=0/child=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2255 [3]: pmixp_coll_tree.c:1342: my peerid: 17:cdr1703
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2255 [3]: pmixp_coll_tree.c:1345: root host: 17:cdr1703
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2255 [3]: pmixp_coll_tree.c:1348: prnt host: 18:cdr1707
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2255 [3]: pmixp_coll_tree.c:1350: prnt contrib:
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2255 [3]: pmixp_coll_tree.c:1351: 	 [18:cdr1707] false
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2255 [3]: pmixp_coll_tree.c:1392: status: coll=COLL_UPFWD upfw=COLL_SND_DONE dfwd=COLL_SND_NONE
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2255 [3]: pmixp_coll_tree.c:1396: dfwd status: dfwd_cb_cnt=0, dfwd_cb_wait=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2255 [3]: pmixp_coll_tree.c:1398: bufs (offset/size): upfw 69/16415, dfwd 69/16415
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_reset_if_to: cdr2242 [2]: pmixp_coll_tree.c:1321: 0x14fb2407a730: collective timeout seq=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_log: cdr2242 [2]: pmixp_coll.c:286: Dumping collective state
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2242 [2]: pmixp_coll_tree.c:1337: 0x14fb2407a730: COLL_FENCE_TREE state seq=1 contribs: loc=1/prnt=0/child=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2242 [2]: pmixp_coll_tree.c:1342: my peerid: 16:cdr2458
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2242 [2]: pmixp_coll_tree.c:1345: root host: 17:cdr1703
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2242 [2]: pmixp_coll_tree.c:1348: prnt host: 18:cdr1707
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2242 [2]: pmixp_coll_tree.c:1350: prnt contrib:
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2242 [2]: pmixp_coll_tree.c:1351: 	 [18:cdr1707] false
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2242 [2]: pmixp_coll_tree.c:1392: status: coll=COLL_UPFWD upfw=COLL_SND_DONE dfwd=COLL_SND_NONE
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2242 [2]: pmixp_coll_tree.c:1396: dfwd status: dfwd_cb_cnt=0, dfwd_cb_wait=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2242 [2]: pmixp_coll_tree.c:1398: bufs (offset/size): upfw 69/16415, dfwd 69/16415
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_reset_if_to: cdr2412 [10]: pmixp_coll_tree.c:1321: 0x14ef70087f20: collective timeout seq=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_log: cdr2412 [10]: pmixp_coll.c:286: Dumping collective state
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2412 [10]: pmixp_coll_tree.c:1337: 0x14ef70087f20: COLL_FENCE_TREE state seq=1 contribs: loc=1/prnt=0/child=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2412 [10]: pmixp_coll_tree.c:1342: my peerid: 24:cdr1736
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2412 [10]: pmixp_coll_tree.c:1345: root host: 17:cdr1703
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2412 [10]: pmixp_coll_tree.c:1348: prnt host: 4:cdr2277
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2412 [10]: pmixp_coll_tree.c:1350: prnt contrib:
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2412 [10]: pmixp_coll_tree.c:1351: 	 [4:cdr2277] false
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2412 [10]: pmixp_coll_tree.c:1392: status: coll=COLL_UPFWD upfw=COLL_SND_DONE dfwd=COLL_SND_NONE
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2412 [10]: pmixp_coll_tree.c:1396: dfwd status: dfwd_cb_cnt=0, dfwd_cb_wait=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2412 [10]: pmixp_coll_tree.c:1398: bufs (offset/size): upfw 69/16415, dfwd 69/16415
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_reset_if_to: cdr2432 [12]: pmixp_coll_tree.c:1321: 0x14e0d008d0a0: collective timeout seq=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_log: cdr2432 [12]: pmixp_coll.c:286: Dumping collective state
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2432 [12]: pmixp_coll_tree.c:1337: 0x14e0d008d0a0: COLL_FENCE_TREE state seq=1 contribs: loc=1/prnt=0/child=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2432 [12]: pmixp_coll_tree.c:1342: my peerid: 26:cdr1746
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2432 [12]: pmixp_coll_tree.c:1345: root host: 17:cdr1703
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2432 [12]: pmixp_coll_tree.c:1348: prnt host: 4:cdr2277
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2432 [12]: pmixp_coll_tree.c:1350: prnt contrib:
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2432 [12]: pmixp_coll_tree.c:1351: 	 [4:cdr2277] false
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2432 [12]: pmixp_coll_tree.c:1392: status: coll=COLL_UPFWD upfw=COLL_SND_DONE dfwd=COLL_SND_NONE
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2432 [12]: pmixp_coll_tree.c:1396: dfwd status: dfwd_cb_cnt=0, dfwd_cb_wait=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2432 [12]: pmixp_coll_tree.c:1398: bufs (offset/size): upfw 69/16415, dfwd 69/16415
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_reset_if_to: cdr2277 [4]: pmixp_coll_tree.c:1321: 0x14ba88089a00: collective timeout seq=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_log: cdr2277 [4]: pmixp_coll.c:286: Dumping collective state
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2277 [4]: pmixp_coll_tree.c:1337: 0x14ba88089a00: COLL_FENCE_TREE state seq=1 contribs: loc=1/prnt=0/child=8
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2277 [4]: pmixp_coll_tree.c:1342: my peerid: 18:cdr1707
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2277 [4]: pmixp_coll_tree.c:1345: root host: 17:cdr1703
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2277 [4]: pmixp_coll_tree.c:1348: prnt host: 17:cdr1703
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2277 [4]: pmixp_coll_tree.c:1350: prnt contrib:
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2277 [4]: pmixp_coll_tree.c:1351: 	 [17:cdr1703] false
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2277 [4]: pmixp_coll_tree.c:1359: child contribs [12]:
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2277 [4]: pmixp_coll_tree.c:1385: 	 done contrib: cdr[2344,2407,2412,2432,2440,2450,2453,2458]
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2277 [4]: pmixp_coll_tree.c:1387: 	 wait contrib: cdr[2297,2357,2363,2423]
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2277 [4]: pmixp_coll_tree.c:1392: status: coll=COLL_COLLECT upfw=COLL_SND_NONE dfwd=COLL_SND_NONE
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2277 [4]: pmixp_coll_tree.c:1396: dfwd status: dfwd_cb_cnt=0, dfwd_cb_wait=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2277 [4]: pmixp_coll_tree.c:1398: bufs (offset/size): upfw 69/16415, dfwd 69/16415
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_reset_if_to: cdr2458 [16]: pmixp_coll_tree.c:1321: 0x14d8dc0a2900: collective timeout seq=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_log: cdr2458 [16]: pmixp_coll.c:286: Dumping collective state
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2458 [16]: pmixp_coll_tree.c:1337: 0x14d8dc0a2900: COLL_FENCE_TREE state seq=1 contribs: loc=1/prnt=0/child=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2458 [16]: pmixp_coll_tree.c:1342: my peerid: 30:cdr1763
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2458 [16]: pmixp_coll_tree.c:1345: root host: 17:cdr1703
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2458 [16]: pmixp_coll_tree.c:1348: prnt host: 4:cdr2277
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2458 [16]: pmixp_coll_tree.c:1350: prnt contrib:
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2458 [16]: pmixp_coll_tree.c:1351: 	 [4:cdr2277] false
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2458 [16]: pmixp_coll_tree.c:1392: status: coll=COLL_UPFWD upfw=COLL_SND_DONE dfwd=COLL_SND_NONE
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2458 [16]: pmixp_coll_tree.c:1396: dfwd status: dfwd_cb_cnt=0, dfwd_cb_wait=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2458 [16]: pmixp_coll_tree.c:1398: bufs (offset/size): upfw 69/16415, dfwd 69/16415
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_reset_if_to: cdr1729 [21]: pmixp_coll_tree.c:1321: 0x153d54081340: collective timeout seq=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_log: cdr1729 [21]: pmixp_coll.c:286: Dumping collective state
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1729 [21]: pmixp_coll_tree.c:1337: 0x153d54081340: COLL_FENCE_TREE state seq=1 contribs: loc=1/prnt=0/child=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1729 [21]: pmixp_coll_tree.c:1342: my peerid: 4:cdr2277
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1729 [21]: pmixp_coll_tree.c:1345: root host: 17:cdr1703
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1729 [21]: pmixp_coll_tree.c:1348: prnt host: 18:cdr1707
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1729 [21]: pmixp_coll_tree.c:1350: prnt contrib:
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1729 [21]: pmixp_coll_tree.c:1351: 	 [18:cdr1707] false
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1729 [21]: pmixp_coll_tree.c:1392: status: coll=COLL_UPFWD upfw=COLL_SND_DONE dfwd=COLL_SND_NONE
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1729 [21]: pmixp_coll_tree.c:1396: dfwd status: dfwd_cb_cnt=0, dfwd_cb_wait=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1729 [21]: pmixp_coll_tree.c:1398: bufs (offset/size): upfw 69/16415, dfwd 69/16415
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_reset_if_to: cdr1746 [26]: pmixp_coll_tree.c:1321: 0x149bc00a5510: collective timeout seq=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_log: cdr1746 [26]: pmixp_coll.c:286: Dumping collective state
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1746 [26]: pmixp_coll_tree.c:1337: 0x149bc00a5510: COLL_FENCE_TREE state seq=1 contribs: loc=1/prnt=0/child=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1746 [26]: pmixp_coll_tree.c:1342: my peerid: 9:cdr2407
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1746 [26]: pmixp_coll_tree.c:1345: root host: 17:cdr1703
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1746 [26]: pmixp_coll_tree.c:1348: prnt host: 18:cdr1707
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1746 [26]: pmixp_coll_tree.c:1350: prnt contrib:
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1746 [26]: pmixp_coll_tree.c:1351: 	 [18:cdr1707] false
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1746 [26]: pmixp_coll_tree.c:1392: status: coll=COLL_UPFWD upfw=COLL_SND_DONE dfwd=COLL_SND_NONE
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1746 [26]: pmixp_coll_tree.c:1396: dfwd status: dfwd_cb_cnt=0, dfwd_cb_wait=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1746 [26]: pmixp_coll_tree.c:1398: bufs (offset/size): upfw 69/16415, dfwd 69/16415
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_reset_if_to: cdr2453 [15]: pmixp_coll_tree.c:1321: 0x14b1440849b0: collective timeout seq=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_log: cdr2453 [15]: pmixp_coll.c:286: Dumping collective state
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2453 [15]: pmixp_coll_tree.c:1337: 0x14b1440849b0: COLL_FENCE_TREE state seq=1 contribs: loc=1/prnt=0/child=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2453 [15]: pmixp_coll_tree.c:1342: my peerid: 29:cdr1762
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2453 [15]: pmixp_coll_tree.c:1345: root host: 17:cdr1703
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2453 [15]: pmixp_coll_tree.c:1348: prnt host: 4:cdr2277
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2453 [15]: pmixp_coll_tree.c:1350: prnt contrib:
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2453 [15]: pmixp_coll_tree.c:1351: 	 [4:cdr2277] false
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2453 [15]: pmixp_coll_tree.c:1392: status: coll=COLL_UPFWD upfw=COLL_SND_DONE dfwd=COLL_SND_NONE
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2453 [15]: pmixp_coll_tree.c:1396: dfwd status: dfwd_cb_cnt=0, dfwd_cb_wait=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2453 [15]: pmixp_coll_tree.c:1398: bufs (offset/size): upfw 69/16415, dfwd 69/16415
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_reset_if_to: cdr1763 [30]: pmixp_coll_tree.c:1321: 0x14f4000821b0: collective timeout seq=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_log: cdr1763 [30]: pmixp_coll.c:286: Dumping collective state
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1763 [30]: pmixp_coll_tree.c:1337: 0x14f4000821b0: COLL_FENCE_TREE state seq=1 contribs: loc=1/prnt=0/child=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1763 [30]: pmixp_coll_tree.c:1342: my peerid: 13:cdr2440
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1763 [30]: pmixp_coll_tree.c:1345: root host: 17:cdr1703
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1763 [30]: pmixp_coll_tree.c:1348: prnt host: 18:cdr1707
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1763 [30]: pmixp_coll_tree.c:1350: prnt contrib:
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1763 [30]: pmixp_coll_tree.c:1351: 	 [18:cdr1707] false
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1763 [30]: pmixp_coll_tree.c:1392: status: coll=COLL_UPFWD upfw=COLL_SND_DONE dfwd=COLL_SND_NONE
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1763 [30]: pmixp_coll_tree.c:1396: dfwd status: dfwd_cb_cnt=0, dfwd_cb_wait=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1763 [30]: pmixp_coll_tree.c:1398: bufs (offset/size): upfw 69/16415, dfwd 69/16415
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_reset_if_to: cdr2091 [0]: pmixp_coll_tree.c:1321: 0x14718c0790f0: collective timeout seq=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_log: cdr2091 [0]: pmixp_coll.c:286: Dumping collective state
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2091 [0]: pmixp_coll_tree.c:1337: 0x14718c0790f0: COLL_FENCE_TREE state seq=1 contribs: loc=1/prnt=0/child=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2091 [0]: pmixp_coll_tree.c:1342: my peerid: 14:cdr2450
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2091 [0]: pmixp_coll_tree.c:1345: root host: 17:cdr1703
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2091 [0]: pmixp_coll_tree.c:1348: prnt host: 18:cdr1707
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2091 [0]: pmixp_coll_tree.c:1350: prnt contrib:
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2091 [0]: pmixp_coll_tree.c:1351: 	 [18:cdr1707] false
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2091 [0]: pmixp_coll_tree.c:1392: status: coll=COLL_UPFWD upfw=COLL_SND_DONE dfwd=COLL_SND_NONE
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2091 [0]: pmixp_coll_tree.c:1396: dfwd status: dfwd_cb_cnt=0, dfwd_cb_wait=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr2091 [0]: pmixp_coll_tree.c:1398: bufs (offset/size): upfw 69/16415, dfwd 69/16415
srun: error: cdr2242: task 2: Exited with exit code 1
srun: Terminating StepId=64636422.1
slurmstepd: error: *** STEP 64636422.1 ON cdr2091 CANCELLED AT 2025-07-24T09:05:18 ***
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_reset_if_to: cdr1739 [25]: pmixp_coll_tree.c:1321: 0x152cf4080d80: collective timeout seq=1
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_log: cdr1739 [25]: pmixp_coll.c:286: Dumping collective state
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1739 [25]: pmixp_coll_tree.c:1337: 0x152cf4080d80: COLL_FENCE_TREE state seq=1 contribs: loc=1/prnt=0/child=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1739 [25]: pmixp_coll_tree.c:1342: my peerid: 8:cdr2363
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1739 [25]: pmixp_coll_tree.c:1345: root host: 17:cdr1703
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1739 [25]: pmixp_coll_tree.c:1348: prnt host: 18:cdr1707
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1739 [25]: pmixp_coll_tree.c:1350: prnt contrib:
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1739 [25]: pmixp_coll_tree.c:1351: 	 [18:cdr1707] false
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1739 [25]: pmixp_coll_tree.c:1392: status: coll=COLL_UPFWD upfw=COLL_SND_DONE dfwd=COLL_SND_NONE
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1739 [25]: pmixp_coll_tree.c:1396: dfwd status: dfwd_cb_cnt=0, dfwd_cb_wait=0
slurmstepd: error:  mpi/pmix_v4: pmixp_coll_tree_log: cdr1739 [25]: pmixp_coll_tree.c:1398: bufs (offset/size): upfw 69/16415, dfwd 69/16415
srun: error: cdr2450: task 14: Exited with exit code 1
srun: error: cdr1717: task 19: Terminated
srun: error: cdr1732: task 22: Terminated
srun: error: cdr2440: task 13: Terminated
srun: error: cdr2412: task 10: Terminated
srun: error: cdr1703: task 17: Terminated
srun: error: cdr1729: task 21: Terminated
srun: error: cdr2458: task 16: Terminated
srun: error: cdr2432: task 12: Terminated
srun: error: cdr2363: task 8: Terminated
srun: error: cdr2255: task 3: Terminated
srun: error: cdr2160: task 1: Terminated
srun: error: cdr1760: task 28: Terminated
srun: error: cdr2344: task 6: Terminated
srun: error: cdr2091: task 0: Terminated
srun: error: cdr2407: task 9: Terminated
srun: error: cdr1707: task 18: Terminated
srun: error: cdr1749: task 27: Terminated
srun: error: cdr1727: task 20: Terminated
srun: error: cdr1735: task 23: Terminated
srun: error: cdr2453: task 15: Terminated
srun: error: cdr2277: task 4: Terminated
srun: error: cdr1763: task 30: Terminated
srun: error: cdr1746: task 26: Terminated
srun: error: cdr2357: task 7: Terminated
srun: error: cdr2423: task 11: Terminated
srun: error: cdr1736: task 24: Terminated
srun: error: cdr2297: task 5: Terminated
srun: error: cdr1739: task 25: Terminated
srun: error: cdr1762: task 29: Terminated
srun: Force Terminated StepId=64636422.1
cp: cannot stat 'federated_metrics.png': No such file or directory
